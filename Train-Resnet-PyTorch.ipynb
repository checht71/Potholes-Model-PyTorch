{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f155260",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801376c-fd1b-441b-a983-377a317a67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5ca60",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae748155-739a-4a79-b844-22d7c20689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the configuration file - standard way - use .env file and load_dotenv from python-dotenv module\n",
    "config_file_path = \"config.json\"\n",
    "\n",
    "# Read JSON data into a dictionary\n",
    "with open(config_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "IMAGE_DIR = data[\"image_dir\"]\n",
    "CSV_DIR = data[\"csv_dir\"]\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "FINE_TUNE_EPOCHS = 30\n",
    "EARLY_STOP_EPOCHS = 3\n",
    "save_model = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.Resize(size=(128, 128)),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc1c8-df54-4fd6-8192-b11bdb2d3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(CSV_DIR)\n",
    "        self.imgs=self.df[['Img_name']]\n",
    "        self.labels=self.df[['label']]\n",
    "        self.imgs.reset_index(drop=True)\n",
    "        self.labels.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #print(f'{IMAGE_DIR}/{self.imgs.iat[idx, 0]}')\n",
    "        rawimg = Image.open(f'{IMAGE_DIR}/{self.imgs.iat[idx, 0]}')\n",
    "        try:\n",
    "            trans_image= transforms(rawimg)\n",
    "            numpyimage = np.array(trans_image)\n",
    "            return numpyimage, self.labels.iat[idx, 0]\n",
    "        except:\n",
    "            print(f\"{self.imgs.iat[idx, 0]} is corrupted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841bec7-1334-4555-880b-8166794b2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = myImageDataset()\n",
    "\n",
    "total_size = len(dataset)\n",
    "val_size = int(0.2 * total_size)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33681e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c82060",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.01, last_epoch=-1, verbose='deprecated')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2724e-e129-462e-b534-74a26df5e152",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bba35-a198-4581-a017-ea31990b295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float() / 255.0\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        labels = labels.to(device)\n",
    "        print(type(inputs))\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        #print(f'loss: {running_loss}')\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    v_correct = 0\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.float() / 255.0\n",
    "            vinputs = vinputs.to(device)\n",
    "            vinputs = vinputs.permute(0, 3, 1, 2)\n",
    "            vlabels = vlabels.to(device)\n",
    "            \n",
    "            voutputs = model(vinputs)\n",
    "            vpredictions = torch.argmax(voutputs, dim=1)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "            for v, vprediction in enumerate(vpredictions):\n",
    "                if vprediction == vlabels[v]:\n",
    "                    v_correct+=1\n",
    "        v_accuracy = round(v_correct/(i*BATCH_SIZE)*100, 2)\n",
    "        print(f'{v_correct}/{i*BATCH_SIZE}')\n",
    "        print(F'Val Accuracy: {v_accuracy}%')\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    return avg_vloss, v_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539107-7a3b-4f39-8477-98e5da935ef1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3948dd-6fed-475a-b204-06d86f5451d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "f_epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d5606-6d55-4548-8cb4-38543db40e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vloss = 1000000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_vloss = validate_one_epoch()\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87910424",
   "metadata": {},
   "source": [
    "#### Unfreeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9671b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_counter = 0\n",
    "\n",
    "for f_epoch in range(FINE_TUNE_EPOCHS):\n",
    "    print('EPOCH {}:'.format(f_epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(f_epoch_number, writer)\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_vloss, v_accuracy = validate_one_epoch()\n",
    "    \n",
    "    print(f'{avg_vloss} vs. {best_vloss}')\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        if save_model == True:\n",
    "            model_path = 'model_{}_{}_{}'.format(timestamp, f_epoch_number+epoch_number, round(v_accuracy))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            torch.save(model, '/home/ubuntu/Dataset/'+ model_path)\n",
    "            early_stop_counter = 0\n",
    "    elif avg_vloss > best_vloss:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= EARLY_STOP_EPOCHS:\n",
    "        print(\"early stopping...\")\n",
    "        model_path = 'model_{}_{}_{}'.format(timestamp, f_epoch_number+epoch_number, round(v_accuracy))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save(model, '/home/ubuntu/Dataset/full_models/'+ model_path)\n",
    "        break\n",
    "    \n",
    "    f_epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
